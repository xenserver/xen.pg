From 452fd1843bebe456497106260cc3121bcb05b0c5 Mon Sep 17 00:00:00 2001
From: Sergey Dyasli <sergey.dyasli@citrix.com>
Date: Thu, 11 May 2017 09:34:40 +0100
Subject: [PATCH] x86/vvmx: add initial PV EPT support in L0

Xen doesn't have the ability to change RWX bits on non leaf EPT entries,
so we just drop all non-leaf requests on the floor. Note this also breaks
something which attempts to unmap 2M, by simply nuking the non-leaf entry
above, also breaks page combining.

Signed-off-by: Sergey Dyasli <sergey.dyasli@citrix.com>
diff --git a/xen/arch/x86/hvm/vmx/vvmx.c b/xen/arch/x86/hvm/vmx/vvmx.c
index 27221fdb733b..ffcf0b392880 100644
--- a/xen/arch/x86/hvm/vmx/vvmx.c
+++ b/xen/arch/x86/hvm/vmx/vvmx.c
@@ -29,6 +29,8 @@
 #include <asm/hvm/vmx/vvmx.h>
 #include <asm/hvm/nestedhvm.h>
 
+#include "../../mm/mm-locks.h"
+
 static DEFINE_PER_CPU(u64 *, vvmcs_buf);
 
 static void nvmx_purge_vvmcs(struct vcpu *v);
@@ -2019,19 +2021,117 @@ static int nvmx_handle_vmwrite(struct cpu_user_regs *regs)
     return X86EMUL_OKAY;
 }
 
+struct pv_invept_desc
+{
+    u64 eptp;
+    union {
+        u64 L2_gpa; /* Metadata in the low 12 bits. */
+        struct {
+            unsigned int lvl:3, valid:1, inv:1;
+        };
+    };
+    u64 L21e;
+};
+
+int pv_invept(struct vcpu *v, const struct pv_invept_desc *desc)
+{
+    struct domain *d = v->domain;
+    struct p2m_domain *L1p2m;
+    struct npfec npfec = {};
+
+    paddr_t L2_gpa = desc->L2_gpa & PAGE_MASK;
+
+    paddr_t L0_gpa;
+    p2m_type_t L10_p2mt;
+    p2m_access_t L10_p2ma = p2m_access_rwx;
+    unsigned int L10_order;
+
+    paddr_t nL1_gpa = desc->L21e & PAGE_MASK;
+    uint8_t nL21_p2ma = desc->L21e & 7;
+    unsigned int nL21_order = desc->lvl * 9;
+
+    p2m_access_t L20_p2ma;
+    unsigned int L20_order;
+
+    int rv;
+
+    if ( !desc->valid )
+        return -EINVAL;
+
+    /* If this is a non-leaf update - drop it */
+    if ( nL21_order && !(desc->L21e & 0x80) )
+        return 0;
+
+    /*
+     * If present is zero, set the nL1_gpa to something that'll always resolve
+     */
+    if ( !nL21_p2ma )
+        nL1_gpa = 0;
+
+    /* check leaf vs non-leaf */
+    /* We assume that all required L1 --> L0 mappings are present */
+    rv = nestedhap_walk_L0_p2m(p2m_get_hostp2m(d), nL1_gpa,
+                               &L0_gpa, &L10_p2mt, &L10_p2ma, &L10_order,
+                               npfec);
+
+    if ( rv != NESTEDHVM_PAGEFAULT_DONE )
+    {
+        gdprintk(XENLOG_ERR, "%s(%pv, { %p, %p, %p })\n",
+                 __func__, v, _p(desc->eptp), _p(desc->L2_gpa), _p(desc->L21e));
+        gdprintk(XENLOG_ERR, "L0walk(%p) => %d - %p/%u/%#x/%#x\n",
+                 _p(nL1_gpa), rv, _p(L0_gpa), L10_order, L10_p2ma, L10_p2mt);
+
+        return -EFAULT;
+    }
+
+    L20_order = min(nL21_order, L10_order);
+    L20_p2ma = nL21_p2ma & L10_p2ma;
+
+    L1p2m = np2m_get_by_base_locked(v, desc->eptp);
+    if ( !L1p2m )
+        return -EFAULT;
+
+    if ( desc->inv )
+    {
+        nestedhap_fix_p2m(v, L1p2m, L2_gpa, L0_gpa, L20_order,
+                          L10_p2mt, L20_p2ma & ~p2m_access_w);
+
+        ept_sync_domain(L1p2m);
+    }
+
+    nestedhap_fix_p2m(v, L1p2m, L2_gpa, L0_gpa, L20_order, L10_p2mt, L20_p2ma);
+    p2m_unlock(L1p2m);
+
+    return 0;
+}
+
 static int nvmx_handle_invept(struct cpu_user_regs *regs)
 {
     struct vmx_inst_decoded decode;
-    unsigned long eptp;
+    pagefault_info_t pfinfo;
     int ret;
 
-    if ( (ret = decode_vmx_inst(regs, &decode, &eptp)) != X86EMUL_OKAY )
+    if ( (ret = decode_vmx_inst(regs, &decode, NULL)) != X86EMUL_OKAY )
         return ret;
 
+    if ( decode.type != VMX_INST_MEMREG_TYPE_MEMORY )
+    {
+        hvm_inject_hw_exception(X86_EXC_UD, 0);
+        return X86EMUL_EXCEPTION;
+    }
+
     switch ( reg_read(regs, decode.reg2) )
     {
     case INVEPT_SINGLE_CONTEXT:
     {
+        unsigned long eptp;
+
+        /* TODO - reject SINGLE_CONTEXT if not configured. */
+
+        ret = hvm_copy_from_guest_linear(&eptp, decode.mem, 8, 0, &pfinfo);
+        if ( ret != HVMTRANS_okay )
+            return X86EMUL_EXCEPTION;
+
         np2m_flush_base(current, eptp);
         break;
     }
@@ -2039,6 +2139,20 @@ static int nvmx_handle_invept(struct cpu_user_regs *regs)
         p2m_flush_nestedp2m(current->domain);
         __invept(INVEPT_ALL_CONTEXT, 0);
         break;
+    case INVEPT_PVEPT_CONTEXT:
+    {
+        struct pv_invept_desc desc;
+
+        ret = hvm_copy_from_guest_linear(&desc, decode.mem, 24, 0, &pfinfo);
+        if ( ret != HVMTRANS_okay )
+            return X86EMUL_EXCEPTION;
+
+        /* top 12 bits are ignored */
+        desc.L21e &= (1UL << 52) - 1;
+
+        pv_invept(current, &desc);
+        break;
+    }
     default:
         vmfail(regs, VMX_INSN_INVEPT_INVVPID_INVALID_OP);
         return X86EMUL_OKAY;
diff --git a/xen/arch/x86/include/asm/hvm/vmx/vmx.h b/xen/arch/x86/include/asm/hvm/vmx/vmx.h
index 82a9487b40f5..66e8fa9eb8b9 100644
--- a/xen/arch/x86/include/asm/hvm/vmx/vmx.h
+++ b/xen/arch/x86/include/asm/hvm/vmx/vmx.h
@@ -307,6 +307,7 @@ extern uint8_t posted_intr_vector;
 
 #define INVEPT_SINGLE_CONTEXT   1
 #define INVEPT_ALL_CONTEXT      2
+#define INVEPT_PVEPT_CONTEXT    0x88d8
 
 #define cpu_has_vmx_vpid_invvpid_individual_addr                    \
     (vmx_ept_vpid_cap & VMX_VPID_INVVPID_INDIVIDUAL_ADDR)
diff --git a/xen/arch/x86/include/asm/p2m.h b/xen/arch/x86/include/asm/p2m.h
index 59e5a8204608..a7992abf5c7d 100644
--- a/xen/arch/x86/include/asm/p2m.h
+++ b/xen/arch/x86/include/asm/p2m.h
@@ -859,11 +859,20 @@ void p2m_flush_nestedp2m(struct domain *d);
 /* Flushes the np2m specified by np2m_base (if it exists) */
 void np2m_flush_base(struct vcpu *v, unsigned long np2m_base);
 
+struct p2m_domain *np2m_get_by_base_locked(struct vcpu *v, uint64_t np2m_base);
+
 void hap_p2m_init(struct p2m_domain *p2m);
 void shadow_p2m_init(struct p2m_domain *p2m);
 
 void cf_check nestedp2m_write_p2m_entry_post(
     struct p2m_domain *p2m, unsigned int oflags);
+void
+nestedhap_fix_p2m(struct vcpu *v, struct p2m_domain *p2m,
+                  paddr_t L2_gpa, paddr_t L0_gpa,
+                  unsigned int page_order, p2m_type_t p2mt, p2m_access_t p2ma);
+int nestedhap_walk_L0_p2m(
+    struct p2m_domain *p2m, paddr_t L1_gpa, paddr_t *L0_gpa, p2m_type_t *p2mt,
+    p2m_access_t *p2ma, unsigned int *page_order, struct npfec npfec);
 
 /*
  * Alternate p2m: shadow p2m tables used for alternate memory views
diff --git a/xen/arch/x86/mm/hap/nested_hap.c b/xen/arch/x86/mm/hap/nested_hap.c
index 098e8e5d4ca9..cc3c3531597e 100644
--- a/xen/arch/x86/mm/hap/nested_hap.c
+++ b/xen/arch/x86/mm/hap/nested_hap.c
@@ -80,7 +80,7 @@ nestedp2m_write_p2m_entry_post(struct p2m_domain *p2m, unsigned int oflags)
 /********************************************/
 /*          NESTED VIRT FUNCTIONS           */
 /********************************************/
-static void
+void
 nestedhap_fix_p2m(struct vcpu *v, struct p2m_domain *p2m, 
                   paddr_t L2_gpa, paddr_t L0_gpa,
                   unsigned int page_order, p2m_type_t p2mt, p2m_access_t p2ma)
@@ -116,7 +116,7 @@ nestedhap_fix_p2m(struct vcpu *v, struct p2m_domain *p2m,
  * walk is successful, the translated value is returned in L0_gpa. The return 
  * value tells the upper level what to do.
  */
-static int nestedhap_walk_L0_p2m(
+int nestedhap_walk_L0_p2m(
     struct p2m_domain *p2m, paddr_t L1_gpa, paddr_t *L0_gpa, p2m_type_t *p2mt,
     p2m_access_t *p2ma, unsigned int *page_order, struct npfec npfec)
 {
diff --git a/xen/arch/x86/mm/p2m.c b/xen/arch/x86/mm/p2m.c
index 7e481aec380a..7e0c8a8c5ec1 100644
--- a/xen/arch/x86/mm/p2m.c
+++ b/xen/arch/x86/mm/p2m.c
@@ -1437,6 +1437,29 @@ void np2m_flush_base(struct vcpu *v, unsigned long np2m_base)
     nestedp2m_unlock(d);
 }
 
+struct p2m_domain *np2m_get_by_base_locked(struct vcpu *v, uint64_t np2m_base)
+{
+    struct domain *d = v->domain;
+    struct p2m_domain *np2m;
+    unsigned int i;
+
+    np2m_base &= ~(0xfffull);
+
+    nestedp2m_lock(d);
+    for ( i = 0; i < MAX_NESTEDP2M; i++ )
+    {
+        np2m = d->arch.nested_p2m[i];
+        p2m_lock(np2m);
+        if ( np2m->np2m_base == np2m_base )
+            break;
+        p2m_unlock(np2m);
+        np2m = NULL;
+    }
+    nestedp2m_unlock(d);
+
+    return np2m;
+}
+
 static void assign_np2m(struct vcpu *v, struct p2m_domain *p2m)
 {
     struct nestedvcpu *nv = &vcpu_nestedhvm(v);
