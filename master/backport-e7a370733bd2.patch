From 813aa07e4cbb913b6a934f177f7f8bcefe08b8d5 Mon Sep 17 00:00:00 2001
From: Sergey Dyasli <sergey.dyasli@citrix.com>
Date: Mon, 25 Sep 2017 10:55:23 +0200
Subject: [PATCH] x86: replace arch_vcpu::cpuid_faulting with msr_vcpu_policy

Since each vCPU now has struct msr_vcpu_policy, use cpuid_faulting bit
from there in current logic and remove arch_vcpu::cpuid_faulting.

Signed-off-by: Sergey Dyasli <sergey.dyasli@citrix.com>
Reviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>
Reviewed-by: Kevin Tian <kevin.tian@intel.com>
diff --git a/xen/arch/x86/cpu/intel.c b/xen/arch/x86/cpu/intel.c
index 3c293c19c4..32211e000f 100644
--- a/xen/arch/x86/cpu/intel.c
+++ b/xen/arch/x86/cpu/intel.c
@@ -161,6 +161,7 @@ static void intel_ctxt_switch_levelling(const struct vcpu *next)
 	struct cpuidmasks *these_masks = &this_cpu(cpuidmasks);
 	const struct domain *nextd = next ? next->domain : NULL;
 	const struct cpuidmasks *masks;
+	const struct msr_vcpu_policy *vp = next->arch.msr;
 
 	if (cpu_has_cpuid_faulting) {
 		/*
@@ -181,7 +182,7 @@ static void intel_ctxt_switch_levelling(const struct vcpu *next)
 		 */
 		set_cpuid_faulting(nextd && !is_control_domain(nextd) &&
 				   (is_pv_domain(nextd) ||
-				    next->arch.cpuid_faulting));
+				    vp->misc_features_enables.cpuid_faulting));
 		return;
 	}
 
diff --git a/xen/arch/x86/hvm/hvm.c b/xen/arch/x86/hvm/hvm.c
index 9f48536f54..f0c1d827a4 100644
--- a/xen/arch/x86/hvm/hvm.c
+++ b/xen/arch/x86/hvm/hvm.c
@@ -3668,9 +3668,10 @@ void hvm_cpuid(unsigned int input, unsigned int *eax, unsigned int *ebx,
 
 bool hvm_check_cpuid_faulting(struct vcpu *v)
 {
+    const struct msr_vcpu_policy *vp = v->arch.msr;
     struct segment_register sreg;
 
-    if ( !v->arch.cpuid_faulting )
+    if ( !vp->misc_features_enables.cpuid_faulting )
         return false;
 
     hvm_get_segment_register(v, x86_seg_ss, &sreg);
diff --git a/xen/arch/x86/hvm/vmx/vmx.c b/xen/arch/x86/hvm/vmx/vmx.c
index 28384b8573..b455b99ea4 100644
--- a/xen/arch/x86/hvm/vmx/vmx.c
+++ b/xen/arch/x86/hvm/vmx/vmx.c
@@ -2888,7 +2888,7 @@ static int vmx_msr_read_intercept(unsigned int msr, uint64_t *msr_content)
 
     case MSR_INTEL_MISC_FEATURES_ENABLES:
         *msr_content = 0;
-        if ( current->arch.cpuid_faulting )
+        if ( current->arch.msr->misc_features_enables.cpuid_faulting )
             *msr_content |= MSR_MISC_FEATURES_CPUID_FAULTING;
         break;
 
@@ -3120,15 +3120,17 @@ static int vmx_msr_write_intercept(unsigned int msr, uint64_t msr_content)
 
     case MSR_INTEL_MISC_FEATURES_ENABLES:
     {
-        bool old_cpuid_faulting = v->arch.cpuid_faulting;
+        struct msr_vcpu_policy *vp = v->arch.msr;
+        bool old_cpuid_faulting = vp->misc_features_enables.cpuid_faulting;
 
         if ( msr_content & ~MSR_MISC_FEATURES_CPUID_FAULTING )
             goto gp_fault;
 
-        v->arch.cpuid_faulting = msr_content & MSR_MISC_FEATURES_CPUID_FAULTING;
+        vp->misc_features_enables.cpuid_faulting =
+            msr_content & MSR_MISC_FEATURES_CPUID_FAULTING;
 
         if ( cpu_has_cpuid_faulting &&
-             (old_cpuid_faulting ^ v->arch.cpuid_faulting) )
+             (old_cpuid_faulting ^ vp->misc_features_enables.cpuid_faulting) )
             ctxt_switch_levelling(v);
         break;
     }
diff --git a/xen/arch/x86/traps.c b/xen/arch/x86/traps.c
index 3e7a4d6100..1dbf1215fa 100644
--- a/xen/arch/x86/traps.c
+++ b/xen/arch/x86/traps.c
@@ -1257,6 +1257,7 @@ static int emulate_forced_invalid_op(struct cpu_user_regs *regs)
 {
     char sig[5], instr[2];
     unsigned long eip, rc;
+    const struct msr_vcpu_policy *vp = current->arch.msr;
 
     eip = regs->eip;
 
@@ -1280,7 +1281,8 @@ static int emulate_forced_invalid_op(struct cpu_user_regs *regs)
         return 0;
 
     /* If cpuid faulting is enabled and CPL>0 inject a #GP in place of #UD. */
-    if ( current->arch.cpuid_faulting && !guest_kernel_mode(current, regs) )
+    if ( vp->misc_features_enables.cpuid_faulting &&
+         !guest_kernel_mode(current, regs) )
     {
         regs->eip = eip;
         do_guest_trap(TRAP_gp_fault, regs, 1);
@@ -2930,7 +2932,7 @@ static int emulate_privileged_op(struct cpu_user_regs *regs)
             if ( (msr_content & MSR_MISC_FEATURES_CPUID_FAULTING) &&
                  !this_cpu(cpuid_faulting_enabled) )
                 goto fail;
-            current->arch.cpuid_faulting =
+            current->arch.msr->misc_features_enables.cpuid_faulting =
                 !!(msr_content & MSR_MISC_FEATURES_CPUID_FAULTING);
             break;
 
@@ -3120,7 +3122,7 @@ static int emulate_privileged_op(struct cpu_user_regs *regs)
                  rdmsr_safe(MSR_INTEL_MISC_FEATURES_ENABLES, val) )
                 goto fail;
             regs->eax = regs->edx = 0;
-            if ( current->arch.cpuid_faulting )
+            if ( current->arch.msr->misc_features_enables.cpuid_faulting )
                 regs->eax |= MSR_MISC_FEATURES_CPUID_FAULTING;
             break;
 
@@ -3183,7 +3185,8 @@ static int emulate_privileged_op(struct cpu_user_regs *regs)
 
     case 0xa2: /* CPUID */
         /* If cpuid faulting is enabled and CPL>0 leave the #GP untouched. */
-        if ( v->arch.cpuid_faulting && !guest_kernel_mode(v, regs) )
+        if ( v->arch.msr->misc_features_enables.cpuid_faulting &&
+             !guest_kernel_mode(v, regs) )
             goto fail;
 
         pv_cpuid(regs);
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index d4642bbd70..dce56ed14f 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -599,9 +599,6 @@ struct arch_vcpu
     /* Restore all FPU state (lazy and non-lazy state) on context switch? */
     bool_t fully_eager_fpu;
 
-    /* Has the guest enabled CPUID faulting? */
-    bool cpuid_faulting;
-
     /*
      * The SMAP check policy when updating runstate_guest(v) and the
      * secondary system time.
