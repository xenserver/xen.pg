diff --git a/xen/common/pv_iommu.c b/xen/common/pv_iommu.c
index 304fccf349a4..a03a37f0f63b 100644
--- a/xen/common/pv_iommu.c
+++ b/xen/common/pv_iommu.c
@@ -17,13 +17,263 @@
  * along with this program; If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <asm/p2m.h>
+#include <asm/event.h>
 #include <xen/guest_access.h>
+#include <public/pv-iommu.h>
 
 #define ret_t long
 
+/*
+ * The bfn_foreign_offset is where the 1:1 BFN:MFN region starts.
+ * This value must be after the Dom0 1:1 PFN:MFN range.
+ * The default for bfn_foreign_offset is calculated as follows:
+ * 1 TB (40 address bits on some GPUs) - 768 G (max host memory that we
+ * want to support and have everything fit under 1 T) - 4 G (MMIO hole)
+ * - 4 G ("wiggle room").
+ */
+
+uint64_t __read_mostly bfn_foreign_offset = (248ULL << 30) >> PAGE_SHIFT;
+size_param("xen_pviommu_foreign_offset", bfn_foreign_offset);
+
+static int get_paged_frame(unsigned long gfn, mfn_t *mfn,
+                           struct page_info **page, int readonly,
+                           struct domain *rd)
+{
+    int rc = 0;
+#if defined(P2M_PAGED_TYPES) || defined(P2M_SHARED_TYPES)
+    p2m_type_t p2mt;
+
+    *page = get_page_from_gfn(rd, gfn, &p2mt,
+                             (readonly) ? P2M_ALLOC : P2M_UNSHARE);
+    if ( !(*page) )
+    {
+        *mfn = INVALID_MFN;
+        if ( p2m_is_shared(p2mt) )
+            return -EIO;
+        if ( p2m_is_paging(p2mt) )
+        {
+            p2m_mem_paging_populate(rd, gfn);
+            return -EIO;
+        }
+        return -EIO;
+    }
+    *mfn = page_to_mfn(*page);
+#else
+    *frame = gmfn_to_mfn(rd, gfn);
+    *page = mfn_valid(_mfn(*frame)) ? mfn_to_page(*frame) : NULL;
+    if ( (!(page)) || (!get_page*page, rd) )
+    {
+        *frame = mfn_x(INVALID_MFN);
+        *page = NULL;
+        rc = -EIO;
+    }
+#endif
+
+    return rc;
+}
+
+int can_use_iommu_check(struct domain *d)
+{
+    if ( !iommu_enabled || (!is_hardware_domain(d) && !is_iommu_enabled(d)) )
+        return 0;
+
+    if ( is_hardware_domain(d) && iommu_hwdom_passthrough )
+        return 0;
+
+    if ( is_hardware_domain(d) && paging_mode_translate(d) )
+        return 0;
+
+    if ( boot_cpu_data.x86_vendor == X86_VENDOR_AMD )
+        return 0;
+
+    return 1;
+}
+
+void do_iommu_sub_op(struct pv_iommu_op *op)
+{
+    struct domain *d = current->domain;
+    struct domain *rd = NULL;
+
+    /* Only order 0 pages supported */
+    if ( IOMMU_get_page_order(op->flags) != 0 )
+    {
+        op->status = -ENOSPC;
+        goto finish;
+    }
+
+    switch ( op->subop_id )
+    {
+        case IOMMUOP_query_caps:
+        {
+            op->flags = 0;
+            op->status = 0;
+            if ( can_use_iommu_check(d) )
+            {
+                op->flags |= IOMMU_QUERY_map_cap | IOMMU_QUERY_map_all_mfns;
+                op->u.query_caps.offset = bfn_foreign_offset;
+            }
+            break;
+        }
+        case IOMMUOP_map_page:
+        {
+            mfn_t mfn, tmp;
+            unsigned int flags;
+            struct page_info *page = NULL;
+
+            /* Check if calling domain can create IOMMU mappings */
+            if ( !can_use_iommu_check(d) )
+            {
+                op->status = -EPERM;
+                goto finish;
+            }
+
+            /* Lookup page struct backing gfn */
+            if ( (op->flags & IOMMU_MAP_OP_no_ref_cnt) )
+            {
+                mfn = _mfn(op->u.map_page.gfn);
+                page = mfn_to_page(mfn);
+                if (!page)
+                {
+                    op->status = -EPERM; // Should this be something else?
+                    goto finish;
+                }
+            } else if ( get_paged_frame(op->u.map_page.gfn, &mfn, &page, 0, d) )
+            {
+                op->status = -EPERM; // Should this be something else?
+                goto finish;
+            }
+
+            /* Check for conflict with existing BFN mappings */
+            if ( !iommu_lookup_page(d, _dfn(op->u.map_page.bfn), &tmp, &flags) )
+            {
+                if ( !(op->flags & IOMMU_MAP_OP_no_ref_cnt) )
+                    put_page(page);
+                op->status = -EPERM;
+                goto finish;
+            }
+
+            flags = 0;
+
+            if ( op->flags & IOMMU_OP_readable )
+                flags |= IOMMUF_readable;
+
+            if ( op->flags & IOMMU_OP_writeable )
+                flags |= IOMMUF_writable;
+
+            if ( iommu_legacy_map(d, _dfn(op->u.map_page.bfn), mfn,
+                                  PAGE_ORDER_4K, flags) )
+            {
+                if ( !(op->flags & IOMMU_MAP_OP_no_ref_cnt) )
+                    put_page(page);
+                op->status = -EIO;
+                goto finish;
+            }
+
+            op->status = 0;
+            break;
+        }
+
+        case IOMMUOP_unmap_page:
+        {
+            struct page_info *page;
+            mfn_t mfn;
+            unsigned int flags;
+
+            /* Check if there is a valid BFN mapping for this domain */
+            if ( iommu_lookup_page(d, _dfn(op->u.unmap_page.bfn), &mfn, &flags) )
+            {
+                op->status = -ENOENT;
+                goto finish;
+            }
+
+            if ( iommu_legacy_unmap(d, _dfn(op->u.unmap_page.bfn),
+                                    PAGE_ORDER_4K) )
+            {
+                op->status = -EIO;
+                goto finish;
+            }
+
+            /* Use MFN from B2M mapping to lookup page */
+            page = mfn_to_page(mfn);
+            if ( !(op->flags & IOMMU_MAP_OP_no_ref_cnt) )
+                put_page(page);
+
+            op->status = 0;
+            break;
+        }
+        default:
+            op->status = -ENODEV;
+            break;
+    }
+
+finish:
+    if ( rd )
+        rcu_unlock_domain(rd);
+
+    return;
+}
+
 ret_t do_iommu_op(XEN_GUEST_HANDLE_PARAM(void) arg, unsigned int count)
 {
-    return -ENOSYS;
+    ret_t ret = 0;
+    int i;
+    struct pv_iommu_op op;
+    struct domain *d = current->domain;
+
+    if ( !is_hardware_domain(d) )
+        return -ENOSYS;
+
+    if ( (int)count < 0 )
+        return -EINVAL;
+
+    if ( count > 1 )
+        this_cpu(iommu_dont_flush_iotlb) = 1;
+
+    for ( i = 0; i < count; i++ )
+    {
+        if ( i && hypercall_preempt_check() )
+        {
+            ret =  i;
+            goto flush_pages;
+        }
+        if ( unlikely(__copy_from_guest_offset(&op, arg, i, 1)) )
+        {
+            ret = -EFAULT;
+            goto flush_pages;
+        }
+        do_iommu_sub_op(&op);
+        if ( unlikely(__copy_to_guest_offset(arg, i, &op, 1)) )
+        {
+            ret = -EFAULT;
+            goto flush_pages;
+        }
+    }
+
+flush_pages:
+    if ( count > 1 )
+    {
+        int rc = 0;
+
+        this_cpu(iommu_dont_flush_iotlb) = 0;
+        if ( i )
+            rc = iommu_iotlb_flush_all(d, IOMMU_FLUSHF_added |
+                                       IOMMU_FLUSHF_modified);
+
+        if ( rc < 0 )
+            ret = rc;
+    }
+    if ( ret > 0 )
+    {
+        XEN_GUEST_HANDLE_PARAM(pv_iommu_op_t) op =
+            guest_handle_cast(arg, pv_iommu_op_t);
+        ASSERT(ret < count);
+        guest_handle_add_offset(op, i);
+        arg = guest_handle_cast(op, void);
+        ret = hypercall_create_continuation(__HYPERVISOR_iommu_op,
+                                           "hi", arg, count - i);
+    }
+    return ret;
 }
 
 /*
diff --git a/xen/drivers/passthrough/x86/iommu.c b/xen/drivers/passthrough/x86/iommu.c
index 818d28f770b3..8db74d5e7762 100644
--- a/xen/drivers/passthrough/x86/iommu.c
+++ b/xen/drivers/passthrough/x86/iommu.c
@@ -285,7 +285,7 @@ static bool __hwdom_init hwdom_iommu_map(const struct domain *d,
 
 void __hwdom_init arch_iommu_hwdom_init(struct domain *d)
 {
-    unsigned long i, top, max_pfn;
+    unsigned long i, top, max_pfn, ram_offset;
     unsigned int flush_flags = 0;
 
     BUG_ON(!is_hardware_domain(d));
@@ -312,6 +312,7 @@ void __hwdom_init arch_iommu_hwdom_init(struct domain *d)
 
     max_pfn = (GB(4) >> PAGE_SHIFT) - 1;
     top = max(max_pdx, pfn_to_pdx(max_pfn) + 1);
+    ram_offset = can_use_iommu_check(d) ? bfn_foreign_offset : 0;
 
     /*
      * First Mb will get mapped in one go by pvh_populate_p2m(). Avoid
@@ -322,15 +323,21 @@ void __hwdom_init arch_iommu_hwdom_init(struct domain *d)
     for ( ; i < top; i++ )
     {
         unsigned long pfn = pdx_to_pfn(i);
+        unsigned long offset = 0;
         int rc;
 
         if ( !hwdom_iommu_map(d, pfn, max_pfn) )
             continue;
 
+        if ( page_get_ram_type(_mfn(pfn)) == RAM_TYPE_CONVENTIONAL )
+            offset = ram_offset;
+        else if ( ram_offset && pfn >= ram_offset )
+            continue;
+
         if ( paging_mode_translate(d) )
             rc = set_identity_p2m_entry(d, pfn, p2m_access_rw, 0);
         else
-            rc = iommu_map(d, _dfn(pfn), _mfn(pfn), PAGE_ORDER_4K,
+            rc = iommu_map(d, _dfn(pfn + offset), _mfn(pfn), PAGE_ORDER_4K,
                            IOMMUF_readable | IOMMUF_writable, &flush_flags);
 
         if ( rc )
diff --git a/xen/include/public/pv-iommu.h b/xen/include/public/pv-iommu.h
new file mode 100644
index 000000000000..f598a8feba76
--- /dev/null
+++ b/xen/include/public/pv-iommu.h
@@ -0,0 +1,73 @@
+/*
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __XEN_PUBLIC_PV_IOMMU_H__
+#define __XEN_PUBLIC_PV_IOMMU_H__
+
+#include "xen.h"
+
+#define IOMMUOP_query_caps            1
+#define IOMMUOP_map_page              2
+#define IOMMUOP_unmap_page            3
+
+struct pv_iommu_op {
+    uint16_t subop_id;
+
+#define IOMMU_page_order (0xf1 << 10)
+#define IOMMU_get_page_order(flags) ((flags & IOMMU_page_order) >> 10)
+#define IOMMU_QUERY_map_cap (1 << 0)
+#define IOMMU_QUERY_map_all_mfns (1 << 1)
+#define IOMMU_OP_readable (1 << 0)
+#define IOMMU_OP_writeable (1 << 1)
+#define IOMMU_MAP_OP_no_ref_cnt (1 << 2)
+    uint16_t flags;
+    int32_t status;
+
+    union {
+        struct {
+            uint64_t offset;
+        } query_caps;
+
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+        } map_page;
+
+        struct {
+            uint64_t bfn;
+        } unmap_page;
+    } u;
+};
+
+
+typedef struct pv_iommu_op pv_iommu_op_t;
+DEFINE_XEN_GUEST_HANDLE(pv_iommu_op_t);
+
+#endif
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/xen/include/xen/iommu.h b/xen/include/xen/iommu.h
index 041a7cf5e50c..327053f4e64d 100644
--- a/xen/include/xen/iommu.h
+++ b/xen/include/xen/iommu.h
@@ -359,6 +359,9 @@ DECLARE_PER_CPU(bool_t, iommu_dont_flush_iotlb);
 extern struct spinlock iommu_pt_cleanup_lock;
 extern struct page_list_head iommu_pt_cleanup_list;
 
+extern uint64_t bfn_foreign_offset;
+int can_use_iommu_check(struct domain *d);
+
 #endif /* _IOMMU_H_ */
 
 /*
