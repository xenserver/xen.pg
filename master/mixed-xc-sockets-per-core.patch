Expose vcpus as multiple cores in a smaller number of sockets,
by adjusting the cpuid responses appropriately.

diff --git a/tools/flask/policy/modules/dom0.te b/tools/flask/policy/modules/dom0.te
index 10e5fbf..2eb645e 100644
--- a/tools/flask/policy/modules/dom0.te
+++ b/tools/flask/policy/modules/dom0.te
@@ -40,7 +40,7 @@ allow dom0_t dom0_t:domain {
 allow dom0_t dom0_t:domain2 {
 	set_cpuid gettsc settsc setscheduler set_max_evtchn set_vnumainfo
 	get_vnumainfo psr_cmt_op psr_alloc set_gnttab_limits
-	get_runstate_info
+	get_runstate_info setcorespersocket
 };
 allow dom0_t dom0_t:resource { add remove };
 
diff --git a/tools/flask/policy/modules/xen.if b/tools/flask/policy/modules/xen.if
index 9c0600f..1f2b41c 100644
--- a/tools/flask/policy/modules/xen.if
+++ b/tools/flask/policy/modules/xen.if
@@ -53,7 +53,7 @@ define(`create_domain_common', `
 	allow $1 $2:domain2 { set_cpuid settsc setscheduler setclaim
 			set_max_evtchn set_vnumainfo get_vnumainfo cacheflush
 			psr_cmt_op psr_alloc soft_reset set_gnttab_limits
-			get_runstate_info
+			get_runstate_info setcorespersocket
 			resource_map };
 	allow $1 $2:security check_context;
 	allow $1 $2:shadow enable;
diff --git a/tools/libxc/include/xenctrl.h b/tools/libxc/include/xenctrl.h
index cda1d97..6d8c5a4 100644
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1379,6 +1379,10 @@ int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 int xc_domain_query_disable_migrate(xc_interface *xch, uint32_t domid,
                                     bool *migration_disabled);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket);
+
 int xc_domain_maximum_gpfn(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns);
 
 int xc_domain_nr_gpfns(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns);
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 96ca2a5..555944e 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -914,6 +914,20 @@ int xc_domain_get_tsc_info(xc_interface *xch,
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_setcorespersocket,
+        .domain = domid,
+        .u.corespersocket.cores_per_socket = cores_per_socket,
+    };
+
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns)
 {
     long rc = do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
diff --git a/xen/arch/x86/cpuid.c b/xen/arch/x86/cpuid.c
index 0e3f551..f44e2fa 100644
--- a/xen/arch/x86/cpuid.c
+++ b/xen/arch/x86/cpuid.c
@@ -703,6 +703,7 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
 {
     const struct domain *d = v->domain;
     const struct cpuid_policy *p = d->arch.cpuid;
+    unsigned int cps = d->cores_per_socket;
 
     *res = EMPTY_LEAF;
 
@@ -819,6 +820,24 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
             /* OSXSAVE clear in policy.  Fast-forward CR4 back in. */
             if ( v->arch.hvm.guest_cr[4] & X86_CR4_OSXSAVE )
                 res->c |= cpufeat_mask(X86_FEATURE_OSXSAVE);
+
+            if ( cps > 0 )
+            {
+                /* to fake out #vcpus per socket first force on HT/MC */
+                res->d |= cpufeat_mask(X86_FEATURE_HTT);
+                /* fake out #vcpus and inform guest of #cores per package */
+                res->b &= 0xFF00FFFF;
+                /*
+                 * This (cps * 2) is wrong, and contrary to the statement in the
+                 * AMD manual.  However, Xen unconditionally offers Intel-style
+                 * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+                 * Enumeration Requirements.
+                 *
+                 * Fake up cores-per-socket as a socket with twice as many cores
+                 * as expected, with every odd core offline.
+                 */
+                res->b |= (((cps * 2) & 0xFF) << 16);
+            }
         }
         else /* PV domain */
         {
@@ -929,6 +948,15 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
         }
         goto common_leaf1_adjustments;
 
+    case 0x4:
+        if ( is_hvm_domain(d) && p->x86_vendor == X86_VENDOR_INTEL && cps > 0 )
+        {
+            /* fake out cores per socket */
+            res->a &= 0x3FFF; /* one thread, one core */
+            res->a |= (((cps * 2) - 1) << 26);
+        }
+        break;
+
     case 0x5:
         /*
          * Leak the hardware MONITOR leaf under the same conditions that the
@@ -1015,6 +1043,9 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
              is_hvm_domain(d) && !hvm_long_mode_active(v) )
             res->d &= ~cpufeat_mask(X86_FEATURE_SYSCALL);
 
+        if ( p->x86_vendor == X86_VENDOR_AMD && cps > 0 && is_hvm_domain(d) )
+            res->c |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);
+
     common_leaf1_adjustments:
         if ( is_hvm_domain(d) )
         {
@@ -1057,6 +1088,23 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
         }
         break;
 
+    case 0x80000008:
+        if ( p->x86_vendor == X86_VENDOR_AMD && cps > 0 )
+        {
+            res->c &= 0xFFFF0F00;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            res->c |= ((cps * 2) - 1) & 0xFF;
+        }
+        break;
+
     case 0x8000001c:
         if ( (v->arch.xcr0 & X86_XCR0_LWP) && cpu_has_svm )
             /* Turn on available bit and other features specified in lwp_cfg. */
diff --git a/xen/common/domctl.c b/xen/common/domctl.c
index d430ae4..c322aab 100644
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -1145,6 +1145,32 @@ long do_domctl(XEN_GUEST_HANDLE_PARAM(xen_domctl_t) u_domctl)
         copyback = 1;
         break;
 
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        unsigned int cps = op->u.corespersocket.cores_per_socket;
+
+        /* Toolstack is permitted to set this value exactly once. */
+        if ( d->cores_per_socket != 0 )
+            ret = -EEXIST;
+
+        /* Only meaningful for HVM domains. */
+        else if ( !is_hvm_domain(d) )
+            ret = -EOPNOTSUPP;
+
+        /* Cores per socket is strictly within the bounds of max_vcpus. */
+        else if ( cps < 1 || cps > d->max_vcpus )
+            ret = -EINVAL;
+
+        /* Cores per socket must exactly divide max_vcpus. */
+        else if ( d->max_vcpus % cps != 0 )
+            ret = -EDOM;
+
+        else
+            d->cores_per_socket = cps;
+
+        break;
+    }
+
     default:
         ret = arch_do_domctl(op, d, u_domctl);
         break;
diff --git a/xen/include/public/domctl.h b/xen/include/public/domctl.h
index 484683e..95d02aa 100644
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -1133,6 +1133,13 @@ DEFINE_XEN_GUEST_HANDLE(xen_domctl_runstate_info_t);
 /* Some vcpus are runnable, some are blocked */
 #define DOMAIN_RUNSTATE_partial_contention 5
 
+struct xen_domctl_corespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_corespersocket xen_domctl_corespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_corespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -1218,6 +1225,7 @@ struct xen_domctl {
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
 #define XEN_DOMCTL_gdbsx_domstatus             1003
+#define XEN_DOMCTL_setcorespersocket           4001
     uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
     domid_t  domain;
     union {
@@ -1266,6 +1274,7 @@ struct xen_domctl {
         struct xen_domctl_set_virq_handler  set_virq_handler;
         struct xen_domctl_set_max_evtchn    set_max_evtchn;
         struct xen_domctl_runstate_info     domain_runstate;
+        struct xen_domctl_corespersocket    corespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_set_broken_page_p2m set_broken_page_p2m;
         struct xen_domctl_cacheflush        cacheflush;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index d0ed693..265e603 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -498,6 +498,8 @@ struct domain
     spinlock_t runstate_lock;
     atomic_t runstate_missed_changes;
     domain_runstate_info_t runstate;
+
+    unsigned int cores_per_socket;
 };
 
 /* Protect updates/reads (resp.) of domain_list and domain_hash. */
diff --git a/xen/xsm/flask/hooks.c b/xen/xsm/flask/hooks.c
index 26a1577..b70c1ac 100644
--- a/xen/xsm/flask/hooks.c
+++ b/xen/xsm/flask/hooks.c
@@ -754,6 +754,9 @@ static int flask_domctl(struct domain *d, int cmd)
     case XEN_DOMCTL_get_runstate_info:
         return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__GET_RUNSTATE_INFO);
 
+    case XEN_DOMCTL_setcorespersocket:
+        return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__SETCORESPERSOCKET);
+
     default:
         return avc_unknown_permission("domctl", cmd);
     }
diff --git a/xen/xsm/flask/policy/access_vectors b/xen/xsm/flask/policy/access_vectors
index 0807ca8..ffd0bb4 100644
--- a/xen/xsm/flask/policy/access_vectors
+++ b/xen/xsm/flask/policy/access_vectors
@@ -256,6 +256,8 @@ class domain2
     resource_map
 # XEN_DOMCTL_get_runstate_info
     get_runstate_info
+# XEN_DOMCTL_setcorespersocket
+    setcorespersocket
 }
 
 # Similar to class domain, but primarily contains domctls related to HVM domains
