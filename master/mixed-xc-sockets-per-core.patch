Expose vcpus as multiple cores in a smaller number of sockets,
by adjusting the cpuid responses appropriately.

diff --git a/tools/flask/policy/modules/dom0.te b/tools/flask/policy/modules/dom0.te
index cb56095..d9cd948 100644
--- a/tools/flask/policy/modules/dom0.te
+++ b/tools/flask/policy/modules/dom0.te
@@ -40,7 +40,7 @@ allow dom0_t dom0_t:domain {
 allow dom0_t dom0_t:domain2 {
 	set_cpuid gettsc settsc setscheduler set_max_evtchn set_vnumainfo
 	get_vnumainfo psr_cmt_op psr_cat_op
-	get_runstate_info
+	get_runstate_info setcorespersocket
 };
 allow dom0_t dom0_t:resource { add remove };
 
diff --git a/tools/flask/policy/modules/xen.if b/tools/flask/policy/modules/xen.if
index 46699bd..8335938 100644
--- a/tools/flask/policy/modules/xen.if
+++ b/tools/flask/policy/modules/xen.if
@@ -52,7 +52,7 @@ define(`create_domain_common', `
 			settime setdomainhandle };
 	allow $1 $2:domain2 { set_cpuid settsc setscheduler setclaim
 			set_max_evtchn set_vnumainfo get_vnumainfo cacheflush
-			get_runstate_info
+			get_runstate_info setcorespersocket
 			psr_cmt_op psr_cat_op soft_reset };
 	allow $1 $2:security check_context;
 	allow $1 $2:shadow enable;
diff --git a/tools/libxc/include/xenctrl.h b/tools/libxc/include/xenctrl.h
index d4b29b8..f6cf3b5 100644
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1313,6 +1313,10 @@ int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 int xc_domain_query_disable_migrate(xc_interface *xch, uint32_t domid,
                                     bool *migration_disabled);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket);
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns);
 
 int xc_domain_nr_gpfns(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns);
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 6333fd2..9d5e5fa 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -825,6 +825,20 @@ int xc_domain_get_tsc_info(xc_interface *xch,
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_setcorespersocket,
+        .domain = domid,
+        .u.corespersocket.cores_per_socket = cores_per_socket,
+    };
+
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid, xen_pfn_t *gpfns)
 {
     long rc = do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
diff --git a/xen/arch/x86/hvm/svm/svm.c b/xen/arch/x86/hvm/svm/svm.c
index 6cc9940..4027cc3 100644
--- a/xen/arch/x86/hvm/svm/svm.c
+++ b/xen/arch/x86/hvm/svm/svm.c
@@ -1606,14 +1606,52 @@ static void svm_cpuid_intercept(
 {
     unsigned int input = *eax;
     struct vcpu *v = current;
+    unsigned int cps = v->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
     switch (input) {
+    case 1:
+        if ( cps > 0 )
+        {
+            /* to fake out #vcpus per socket first force on HT/MC */
+            *edx |= cpufeat_mask(X86_FEATURE_HTT);
+            /* fake out #vcpus and inform guest of #cores per package */
+            *ebx &= 0xFF00FFFF;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            *ebx |= (((cps * 2) & 0xFF) << 16);
+        }
+        break;
     case 0x80000001:
         /* Fix up VLAPIC details. */
         if ( vlapic_hw_disabled(vcpu_vlapic(v)) )
             __clear_bit(X86_FEATURE_APIC & 31, edx);
+        if ( cps > 0 )
+            *ecx |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);
+        break;
+    case 0x80000008:
+        if ( cps > 0 )
+        {
+            *ecx &= 0xFFFF0F00;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            *ecx |= ((cps * 2) - 1) & 0xFF;
+        }
         break;
     case 0x8000001c: 
     {
diff --git a/xen/arch/x86/hvm/vmx/vmx.c b/xen/arch/x86/hvm/vmx/vmx.c
index 7ad22dd..fadf060 100644
--- a/xen/arch/x86/hvm/vmx/vmx.c
+++ b/xen/arch/x86/hvm/vmx/vmx.c
@@ -2539,11 +2539,32 @@ static void vmx_cpuid_intercept(
 {
     unsigned int input = *eax;
     struct vcpu *v = current;
+    unsigned int cps = v->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
     switch ( input )
     {
+        case 0x00000001:
+            if ( cps > 0 )
+            {
+                /* to fake out #vcpus per socket first force on HT/MC */
+                *edx |= cpufeat_mask(X86_FEATURE_HTT);
+                /* fake out #vcpus and inform guest of #cores per package */
+                *ebx &= 0xFF00FFFF;
+                *ebx |= (((cps * 2) & 0xFF) << 16);
+            }
+            break;
+
+        case 0x00000004:
+            if ( cps > 0 )
+            {
+                /* fake out cores per socket */
+                *eax &= 0x3FFF; /* one thread, one core */
+                *eax |= (((cps * 2) - 1) << 26);
+            }
+            break;
+
         case 0x80000001:
             /* SYSCALL is visible iff running in long mode. */
             if ( hvm_long_mode_enabled(v) )
diff --git a/xen/common/domctl.c b/xen/common/domctl.c
index 844e227..596b19b 100644
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -1156,6 +1156,32 @@ long do_domctl(XEN_GUEST_HANDLE_PARAM(xen_domctl_t) u_domctl)
         copyback = 1;
         break;
 
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        unsigned int cps = op->u.corespersocket.cores_per_socket;
+
+        /* Toolstack is permitted to set this value exactly once. */
+        if ( d->cores_per_socket != 0 )
+            ret = -EEXIST;
+
+        /* Only meaningful for HVM domains. */
+        else if ( !has_hvm_container_domain(d) )
+            ret = -EOPNOTSUPP;
+
+        /* Cores per socket is strictly within the bounds of max_vcpus. */
+        else if ( cps < 1 || cps > d->max_vcpus )
+            ret = -EINVAL;
+
+        /* Cores per socket must exactly divide max_vcpus. */
+        else if ( d->max_vcpus % cps != 0 )
+            ret = -EDOM;
+
+        else
+            d->cores_per_socket = cps;
+
+        break;
+    }
+
     default:
         ret = arch_do_domctl(op, d, u_domctl);
         break;
diff --git a/xen/include/public/domctl.h b/xen/include/public/domctl.h
index 6ed7e65..efad119 100644
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -1171,6 +1171,13 @@ DEFINE_XEN_GUEST_HANDLE(xen_domctl_runstate_info_t);
 /* Some vcpus are runnable, some are blocked */
 #define DOMAIN_RUNSTATE_partial_contention 5
 
+struct xen_domctl_corespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_corespersocket xen_domctl_corespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_corespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -1254,6 +1261,7 @@ struct xen_domctl {
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
 #define XEN_DOMCTL_gdbsx_domstatus             1003
+#define XEN_DOMCTL_setcorespersocket           4001
     uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
     domid_t  domain;
     union {
@@ -1303,6 +1311,7 @@ struct xen_domctl {
         struct xen_domctl_set_virq_handler  set_virq_handler;
         struct xen_domctl_set_max_evtchn    set_max_evtchn;
         struct xen_domctl_runstate_info     domain_runstate;
+        struct xen_domctl_corespersocket    corespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_set_broken_page_p2m set_broken_page_p2m;
         struct xen_domctl_cacheflush        cacheflush;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index edf0e4a..1e7386a 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -481,6 +481,8 @@ struct domain
     spinlock_t runstate_lock;
     atomic_t runstate_missed_changes;
     domain_runstate_info_t runstate;
+
+    unsigned int cores_per_socket;
 };
 
 /* Protect updates/reads (resp.) of domain_list and domain_hash. */
diff --git a/xen/xsm/flask/hooks.c b/xen/xsm/flask/hooks.c
index 6aaeaad..835c875 100644
--- a/xen/xsm/flask/hooks.c
+++ b/xen/xsm/flask/hooks.c
@@ -751,6 +751,9 @@ static int flask_domctl(struct domain *d, int cmd)
     case XEN_DOMCTL_get_runstate_info:
         return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__GET_RUNSTATE_INFO);
 
+    case XEN_DOMCTL_setcorespersocket:
+        return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__SETCORESPERSOCKET);
+
     default:
         return avc_unknown_permission("domctl", cmd);
     }
diff --git a/xen/xsm/flask/policy/access_vectors b/xen/xsm/flask/policy/access_vectors
index 11f8ef9..2dfbd9a 100644
--- a/xen/xsm/flask/policy/access_vectors
+++ b/xen/xsm/flask/policy/access_vectors
@@ -250,6 +250,8 @@ class domain2
     psr_cat_op
 # XEN_DOMCTL_get_runstate_info
     get_runstate_info
+# XEN_DOMCTL_setcorespersocket
+    setcorespersocket
 }
 
 # Similar to class domain, but primarily contains domctls related to HVM domains
