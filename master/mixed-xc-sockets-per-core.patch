Expose vcpus as multiple cores in a smaller number of sockets,
by adjusting the cpuid responses appropriately.

diff --git a/tools/flask/policy/modules/dom0.te b/tools/flask/policy/modules/dom0.te
index 0eda6a5936..8aa72fd4b5 100644
--- a/tools/flask/policy/modules/dom0.te
+++ b/tools/flask/policy/modules/dom0.te
@@ -40,7 +40,7 @@ allow dom0_t dom0_t:domain {
 allow dom0_t dom0_t:domain2 {
 	set_cpu_policy gettsc settsc setscheduler set_vnumainfo
 	get_vnumainfo psr_cmt_op psr_alloc get_cpu_policy
-	get_runstate_info
+	get_runstate_info setcorespersocket
 };
 allow dom0_t dom0_t:resource { add remove };
 
diff --git a/tools/flask/policy/modules/xen.if b/tools/flask/policy/modules/xen.if
index 8c915b8381..3e07f4ca26 100644
--- a/tools/flask/policy/modules/xen.if
+++ b/tools/flask/policy/modules/xen.if
@@ -53,7 +53,7 @@ define(`create_domain_common', `
 	allow $1 $2:domain2 { set_cpu_policy settsc setscheduler setclaim
 			set_vnumainfo get_vnumainfo cacheflush
 			psr_cmt_op psr_alloc soft_reset
-			get_runstate_info
+			get_runstate_info setcorespersocket
 			resource_map get_cpu_policy };
 	allow $1 $2:security check_context;
 	allow $1 $2:shadow enable;
diff --git a/tools/libxc/include/xenctrl.h b/tools/libxc/include/xenctrl.h
index ca1ddb11df..6d0ac8e8c8 100644
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1359,6 +1359,10 @@ int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 int xc_domain_query_disable_migrate(xc_interface *xch, uint32_t domid,
                                     bool *migration_disabled);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket);
+
 int xc_domain_maximum_gpfn(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns);
 
 int xc_domain_nr_gpfns(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns);
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 83e841e198..5c43843d90 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -892,6 +892,20 @@ int xc_domain_get_tsc_info(xc_interface *xch,
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_setcorespersocket,
+        .domain = domid,
+        .u.corespersocket.cores_per_socket = cores_per_socket,
+    };
+
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, uint32_t domid, xen_pfn_t *gpfns)
 {
     long rc = do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
diff --git a/xen/arch/x86/cpuid.c b/xen/arch/x86/cpuid.c
index 1a0c5d0fb7..2ae91af9f8 100644
--- a/xen/arch/x86/cpuid.c
+++ b/xen/arch/x86/cpuid.c
@@ -637,6 +637,7 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
 {
     const struct domain *d = v->domain;
     const struct cpuid_policy *p = d->arch.cpuid;
+    unsigned int cps = d->cores_per_socket;
 
     *res = EMPTY_LEAF;
 
@@ -760,6 +761,24 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
             /* OSXSAVE clear in policy.  Fast-forward CR4 back in. */
             if ( v->arch.hvm.guest_cr[4] & X86_CR4_OSXSAVE )
                 res->c |= cpufeat_mask(X86_FEATURE_OSXSAVE);
+
+            if ( cps > 0 )
+            {
+                /* to fake out #vcpus per socket first force on HT/MC */
+                res->d |= cpufeat_mask(X86_FEATURE_HTT);
+                /* fake out #vcpus and inform guest of #cores per package */
+                res->b &= 0xFF00FFFF;
+                /*
+                 * This (cps * 2) is wrong, and contrary to the statement in the
+                 * AMD manual.  However, Xen unconditionally offers Intel-style
+                 * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+                 * Enumeration Requirements.
+                 *
+                 * Fake up cores-per-socket as a socket with twice as many cores
+                 * as expected, with every odd core offline.
+                 */
+                res->b |= (((cps * 2) & 0xFF) << 16);
+            }
         }
         else /* PV domain */
         {
@@ -872,6 +891,15 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
         }
         goto common_leaf1_adjustments;
 
+    case 0x4:
+        if ( is_hvm_domain(d) && p->x86_vendor == X86_VENDOR_INTEL && cps > 0 )
+        {
+            /* fake out cores per socket */
+            res->a &= 0x3FFF; /* one thread, one core */
+            res->a |= (((cps * 2) - 1) << 26);
+        }
+        break;
+
     case 0x5:
         /*
          * Leak the hardware MONITOR leaf under the same conditions that the
@@ -958,6 +986,9 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
              is_hvm_domain(d) && !hvm_long_mode_active(v) )
             res->d &= ~cpufeat_mask(X86_FEATURE_SYSCALL);
 
+        if ( p->x86_vendor == X86_VENDOR_AMD && cps > 0 && is_hvm_domain(d) )
+            res->c |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);
+
     common_leaf1_adjustments:
         if ( is_hvm_domain(d) )
         {
@@ -999,6 +1030,23 @@ void guest_cpuid(const struct vcpu *v, uint32_t leaf,
                 res->d |= cpufeat_mask(X86_FEATURE_MTRR);
         }
         break;
+
+    case 0x80000008:
+        if ( p->x86_vendor == X86_VENDOR_AMD && cps > 0 )
+        {
+            res->c &= 0xFFFF0F00;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            res->c |= ((cps * 2) - 1) & 0xFF;
+        }
+        break;
     }
 }
 
diff --git a/xen/common/domctl.c b/xen/common/domctl.c
index e07674e090..fbc8e4787b 100644
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -1061,6 +1061,32 @@ long do_domctl(XEN_GUEST_HANDLE_PARAM(xen_domctl_t) u_domctl)
         copyback = 1;
         break;
 
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        unsigned int cps = op->u.corespersocket.cores_per_socket;
+
+        /* Toolstack is permitted to set this value exactly once. */
+        if ( d->cores_per_socket != 0 )
+            ret = -EEXIST;
+
+        /* Only meaningful for HVM domains. */
+        else if ( !is_hvm_domain(d) )
+            ret = -EOPNOTSUPP;
+
+        /* Cores per socket is strictly within the bounds of max_vcpus. */
+        else if ( cps < 1 || cps > d->max_vcpus )
+            ret = -EINVAL;
+
+        /* Cores per socket must exactly divide max_vcpus. */
+        else if ( d->max_vcpus % cps != 0 )
+            ret = -EDOM;
+
+        else
+            d->cores_per_socket = cps;
+
+        break;
+    }
+
     default:
         ret = arch_do_domctl(op, d, u_domctl);
         break;
diff --git a/xen/include/public/domctl.h b/xen/include/public/domctl.h
index 7aa5db0c09..d1526561c9 100644
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -1162,6 +1162,13 @@ DEFINE_XEN_GUEST_HANDLE(xen_domctl_runstate_info_t);
 /* Some vcpus are runnable, some are blocked */
 #define DOMAIN_RUNSTATE_partial_contention 5
 
+struct xen_domctl_corespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_corespersocket xen_domctl_corespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_corespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -1249,6 +1256,7 @@ struct xen_domctl {
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
 #define XEN_DOMCTL_gdbsx_domstatus             1003
+#define XEN_DOMCTL_setcorespersocket           4001
     uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
     domid_t  domain;
     union {
@@ -1296,6 +1304,7 @@ struct xen_domctl {
         struct xen_domctl_audit_p2m         audit_p2m;
         struct xen_domctl_set_virq_handler  set_virq_handler;
         struct xen_domctl_runstate_info     domain_runstate;
+        struct xen_domctl_corespersocket    corespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_set_broken_page_p2m set_broken_page_p2m;
         struct xen_domctl_cacheflush        cacheflush;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index 4f9572ba3e..e00d1d917f 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -536,6 +536,8 @@ struct domain
     spinlock_t runstate_lock;
     atomic_t runstate_missed_changes;
     domain_runstate_info_t runstate;
+
+    unsigned int cores_per_socket;
 };
 
 /* Protect updates/reads (resp.) of domain_list and domain_hash. */
diff --git a/xen/xsm/flask/hooks.c b/xen/xsm/flask/hooks.c
index 52f6afbfd3..3da0de6180 100644
--- a/xen/xsm/flask/hooks.c
+++ b/xen/xsm/flask/hooks.c
@@ -750,6 +750,9 @@ static int flask_domctl(struct domain *d, int cmd)
     case XEN_DOMCTL_get_runstate_info:
         return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__GET_RUNSTATE_INFO);
 
+    case XEN_DOMCTL_setcorespersocket:
+        return current_has_perm(d, SECCLASS_DOMAIN2, DOMAIN2__SETCORESPERSOCKET);
+
     default:
         return avc_unknown_permission("domctl", cmd);
     }
diff --git a/xen/xsm/flask/policy/access_vectors b/xen/xsm/flask/policy/access_vectors
index 4ed4e435d1..6377d5aa9d 100644
--- a/xen/xsm/flask/policy/access_vectors
+++ b/xen/xsm/flask/policy/access_vectors
@@ -247,6 +247,8 @@ class domain2
     resource_map
 # XEN_DOMCTL_get_runstate_info
     get_runstate_info
+# XEN_DOMCTL_setcorespersocket
+    setcorespersocket
 # XEN_DOMCTL_get_cpu_policy
     get_cpu_policy
 }
