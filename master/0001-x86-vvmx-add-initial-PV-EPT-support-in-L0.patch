From 452fd1843bebe456497106260cc3121bcb05b0c5 Mon Sep 17 00:00:00 2001
From: Sergey Dyasli <sergey.dyasli@citrix.com>
Date: Thu, 11 May 2017 09:34:40 +0100
Subject: [PATCH] x86/vvmx: add initial PV EPT support in L0

Xen doesn't have the ability to change RWX bits on non leaf EPT entries,
so we just drop all non-leaf requests on the floor. Note this also breaks
something which attempts to unmap 2M, by simply nuking the non-leaf entry
above, also breaks page combining.

Signed-off-by: Sergey Dyasli <sergey.dyasli@citrix.com>
diff --git a/xen/arch/x86/hvm/vmx/vvmx.c b/xen/arch/x86/hvm/vmx/vvmx.c
index 252fbf6..76805cb 100644
--- a/xen/arch/x86/hvm/vmx/vvmx.c
+++ b/xen/arch/x86/hvm/vmx/vvmx.c
@@ -27,6 +27,8 @@
 #include <asm/hvm/vmx/vvmx.h>
 #include <asm/hvm/nestedhvm.h>
 
+#include "../../mm/mm-locks.h"
+
 static DEFINE_PER_CPU(u64 *, vvmcs_buf);
 
 static void nvmx_purge_vvmcs(struct vcpu *v);
@@ -1918,19 +1920,129 @@ static int nvmx_handle_vmwrite(struct cpu_user_regs *regs)
     return X86EMUL_OKAY;
 }
 
+void
+nestedhap_fix_p2m(struct vcpu *v, struct p2m_domain *p2m,
+                  paddr_t L2_gpa, paddr_t L0_gpa,
+                  unsigned int page_order, p2m_type_t p2mt, p2m_access_t p2ma);
+int
+nestedhap_walk_L0_p2m(struct p2m_domain *p2m, paddr_t L1_gpa, paddr_t *L0_gpa,
+                      p2m_type_t *p2mt, p2m_access_t *p2ma,
+                      unsigned int *page_order,
+                      bool_t access_r, bool_t access_w, bool_t access_x);
+void ept_sync_domain_arg(struct p2m_domain *p2m, bool pv_ept);
+
+struct p2m_domain *np2m_get_by_base_locked(struct vcpu *v, uint64_t np2m_base);
+
+struct pv_invept_desc
+{
+    u64 eptp;
+    union {
+        u64 L2_gpa; /* Metadata in the low 12 bits. */
+        struct {
+            unsigned int lvl:3, valid:1, inv:1;
+        };
+    };
+    u64 L21e;
+};
+
+int pv_invept(struct vcpu *v, const struct pv_invept_desc *desc)
+{
+    struct domain *d = v->domain;
+    struct p2m_domain *L1p2m;
+
+    paddr_t L2_gpa = desc->L2_gpa & PAGE_MASK;
+
+    paddr_t L0_gpa;
+    p2m_type_t L10_p2mt;
+    p2m_access_t L10_p2ma = p2m_access_rwx;
+    unsigned int L10_order;
+
+    paddr_t nL1_gpa = desc->L21e & PAGE_MASK;
+    uint8_t nL21_p2ma = desc->L21e & 7;
+    unsigned int nL21_order = desc->lvl * 9;
+
+    p2m_access_t L20_p2ma;
+    unsigned int L20_order;
+
+    int rv;
+
+    if ( !desc->valid )
+        return -EINVAL;
+
+    /* If this is a non-leaf update - drop it */
+    if ( nL21_order && !(desc->L21e & 0x80) )
+        return 0;
+
+    /*
+     * If present is zero, set the nL1_gpa to something that'll always resolve
+     */
+    if ( !nL21_p2ma )
+        nL1_gpa = 0;
+
+    /* check leaf vs non-leaf */
+    /* We assume that all required L1 --> L0 mappings are present */
+    rv = nestedhap_walk_L0_p2m(p2m_get_hostp2m(d), nL1_gpa,
+                               &L0_gpa, &L10_p2mt, &L10_p2ma, &L10_order,
+                               false, false, false);
+
+    if ( rv != NESTEDHVM_PAGEFAULT_DONE )
+    {
+        gdprintk(XENLOG_ERR, "%s(%pv, { %p, %p, %p })\n",
+                 __func__, v, _p(desc->eptp), _p(desc->L2_gpa), _p(desc->L21e));
+        gdprintk(XENLOG_ERR, "L0walk(%p) => %d - %p/%u/%#x/%#x\n",
+                 _p(nL1_gpa), rv, _p(L0_gpa), L10_order, L10_p2ma, L10_p2mt);
+
+        return -EFAULT;
+    }
+
+    L20_order = min(nL21_order, L10_order);
+    L20_p2ma = nL21_p2ma & L10_p2ma;
+
+    L1p2m = np2m_get_by_base_locked(v, desc->eptp);
+    if ( !L1p2m )
+        return -EFAULT;
+
+    if ( desc->inv )
+    {
+        nestedhap_fix_p2m(v, L1p2m, L2_gpa, L0_gpa, L20_order,
+                          L10_p2mt, L20_p2ma & ~p2m_access_w);
+
+        ept_sync_domain(L1p2m);
+    }
+
+    nestedhap_fix_p2m(v, L1p2m, L2_gpa, L0_gpa, L20_order, L10_p2mt, L20_p2ma);
+    p2m_unlock(L1p2m);
+
+    return 0;
+}
+
 static int nvmx_handle_invept(struct cpu_user_regs *regs)
 {
     struct vmx_inst_decoded decode;
-    unsigned long eptp;
+    pagefault_info_t pfinfo;
     int ret;
 
-    if ( (ret = decode_vmx_inst(regs, &decode, &eptp)) != X86EMUL_OKAY )
+    if ( (ret = decode_vmx_inst(regs, &decode, NULL)) != X86EMUL_OKAY )
         return ret;
 
+    if ( decode.type != VMX_INST_MEMREG_TYPE_MEMORY )
+    {
+        hvm_inject_hw_exception(TRAP_invalid_op, 0);
+        return X86EMUL_EXCEPTION;
+    }
+
     switch ( reg_read(regs, decode.reg2) )
     {
     case INVEPT_SINGLE_CONTEXT:
     {
+        unsigned long eptp;
+
+        /* TODO - reject SINGLE_CONTEXT if not configured. */
+
+        ret = hvm_copy_from_guest_linear(&eptp, decode.mem, 8, 0, &pfinfo);
+        if ( ret != HVMTRANS_okay )
+            return X86EMUL_EXCEPTION;
+
         np2m_flush_base(current, eptp);
         break;
     }
@@ -1938,6 +2050,20 @@ static int nvmx_handle_invept(struct cpu_user_regs *regs)
         p2m_flush_nestedp2m(current->domain);
         __invept(INVEPT_ALL_CONTEXT, 0);
         break;
+    case INVEPT_PVEPT_CONTEXT:
+    {
+        struct pv_invept_desc desc;
+
+        ret = hvm_copy_from_guest_linear(&desc, decode.mem, 24, 0, &pfinfo);
+        if ( ret != HVMTRANS_okay )
+            return X86EMUL_EXCEPTION;
+
+        /* top 12 bits are ignored */
+        desc.L21e &= (1UL << 52) - 1;
+
+        pv_invept(current, &desc);
+        break;
+    }
     default:
         vmfail(regs, VMX_INSN_INVEPT_INVVPID_INVALID_OP);
         return X86EMUL_OKAY;
diff --git a/xen/arch/x86/mm/hap/nested_hap.c b/xen/arch/x86/mm/hap/nested_hap.c
index d2a07a5..f37b73e 100644
--- a/xen/arch/x86/mm/hap/nested_hap.c
+++ b/xen/arch/x86/mm/hap/nested_hap.c
@@ -92,7 +92,7 @@ nestedp2m_write_p2m_entry(struct p2m_domain *p2m, unsigned long gfn,
 /********************************************/
 /*          NESTED VIRT FUNCTIONS           */
 /********************************************/
-static void
+void
 nestedhap_fix_p2m(struct vcpu *v, struct p2m_domain *p2m, 
                   paddr_t L2_gpa, paddr_t L0_gpa,
                   unsigned int page_order, p2m_type_t p2mt, p2m_access_t p2ma)
@@ -144,7 +144,7 @@ nestedhap_walk_L1_p2m(struct vcpu *v, paddr_t L2_gpa, paddr_t *L1_gpa,
  * walk is successful, the translated value is returned in L0_gpa. The return 
  * value tells the upper level what to do.
  */
-static int
+int
 nestedhap_walk_L0_p2m(struct p2m_domain *p2m, paddr_t L1_gpa, paddr_t *L0_gpa,
                       p2m_type_t *p2mt, p2m_access_t *p2ma,
                       unsigned int *page_order,
diff --git a/xen/arch/x86/mm/p2m.c b/xen/arch/x86/mm/p2m.c
index 6a1abb6..0fa1c00 100644
--- a/xen/arch/x86/mm/p2m.c
+++ b/xen/arch/x86/mm/p2m.c
@@ -1827,6 +1827,29 @@ void np2m_flush_base(struct vcpu *v, unsigned long np2m_base)
     nestedp2m_unlock(d);
 }
 
+struct p2m_domain *np2m_get_by_base_locked(struct vcpu *v, uint64_t np2m_base)
+{
+    struct domain *d = v->domain;
+    struct p2m_domain *np2m;
+    unsigned int i;
+
+    np2m_base &= ~(0xfffull);
+
+    nestedp2m_lock(d);
+    for ( i = 0; i < MAX_NESTEDP2M; i++ )
+    {
+        np2m = d->arch.nested_p2m[i];
+        p2m_lock(np2m);
+        if ( np2m->np2m_base == np2m_base )
+            break;
+        p2m_unlock(np2m);
+        np2m = NULL;
+    }
+    nestedp2m_unlock(d);
+
+    return np2m;
+}
+
 static void assign_np2m(struct vcpu *v, struct p2m_domain *p2m)
 {
     struct nestedvcpu *nv = &vcpu_nestedhvm(v);
diff --git a/xen/include/asm-x86/hvm/vmx/vmx.h b/xen/include/asm-x86/hvm/vmx/vmx.h
index b110e16..e28c12e 100644
--- a/xen/include/asm-x86/hvm/vmx/vmx.h
+++ b/xen/include/asm-x86/hvm/vmx/vmx.h
@@ -305,6 +305,7 @@ extern uint8_t posted_intr_vector;
 
 #define INVEPT_SINGLE_CONTEXT   1
 #define INVEPT_ALL_CONTEXT      2
+#define INVEPT_PVEPT_CONTEXT    0x88d8
 
 #define cpu_has_vmx_vpid_invvpid_individual_addr                    \
     (vmx_ept_vpid_cap & VMX_VPID_INVVPID_INDIVIDUAL_ADDR)
