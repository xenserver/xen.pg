From 73584ca85664f1335432a1286f8473de53b27182 Mon Sep 17 00:00:00 2001
From: Andrew Cooper <andrew.cooper3@citrix.com>
Date: Fri, 3 Nov 2017 16:39:42 +0000
Subject: [PATCH] x86/entry: Clobber the Return Stack Buffer on entry to Xen

ret instructions are unconditionally speculated based on values in the RSB.
If any path in Xen executes more ret than call instructions, speculation can
start following a guest controlled RSB entry.

There is at least one path (wake from waitqueue) which can end up executing
more ret than call instructions.  There may be other paths as well.

To mitigate, overwrite the RSB (when appropriate; see code for details) when
entering Xen from guest context.

Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
diff --git a/docs/misc/xen-command-line.markdown b/docs/misc/xen-command-line.markdown
index 53a0d1f..df3c697 100644
--- a/docs/misc/xen-command-line.markdown
+++ b/docs/misc/xen-command-line.markdown
@@ -245,7 +245,7 @@ enough. Setting this to a high value may cause boot failure, particularly if
 the NMI watchdog is also enabled.
 
 ### bti (x86)
-> `= List of [ thunk=retpoline|lfence|plain, ibrs=<bool> ]`
+> `= List of [ thunk=retpoline|lfence|plain, ibrs=<bool>, rsb_{vmexit,native}=bool ]`
 
 Branch Target Injection controls.  By default, Xen will pick the most
 appropriate BTI mitigations based on compiled in support, loaded microcode,
@@ -262,6 +262,10 @@ On hardware supporting IBRS, the `ibrs=` option can be used to force or
 prevent Xen using the feature itself.  If Xen is not using IBRS itself,
 functionality is still set up so IBRS can be virtualised for guests.
 
+The `rsb_vmexit=` and `rsb_native=` options can be used to fine tune when the
+RSB gets overwritten.  There are individual controls for an entry from HVM
+context, and an entry from a native (PV or Xen) context.
+
 ### xenheap\_megabytes (arm32)
 > `= <size>`
 
diff --git a/xen/arch/x86/spec_ctrl.c b/xen/arch/x86/spec_ctrl.c
index 5cd9f80..a2da729 100644
--- a/xen/arch/x86/spec_ctrl.c
+++ b/xen/arch/x86/spec_ctrl.c
@@ -34,6 +34,7 @@ enum ind_thunk {
     THUNK_JMP,
 } opt_thunk __initdata = THUNK_DEFAULT;
 int opt_ibrs __initdata = -1;
+int opt_rsb_native __initdata = -1, opt_rsb_vmexit __initdata = -1;
 
 static int __init parse_bti(const char *s)
 {
@@ -60,6 +61,10 @@ static int __init parse_bti(const char *s)
         }
         else if ( (val = parse_boolean("ibrs", s, ss)) >= 0 )
             opt_ibrs = val;
+        else if ( (val = parse_boolean("rsb_native", s, ss)) >= 0 )
+            opt_rsb_native = val;
+        else if ( (val = parse_boolean("rsb_vmexit", s, ss)) >= 0 )
+            opt_rsb_vmexit = val;
         else
             rc = -EINVAL;
 
@@ -72,21 +77,23 @@ custom_param("bti", parse_bti);
 
 static void __init print_details(enum ind_thunk thunk)
 {
-    unsigned int _7d0 = 0, e8b = 0, tmp;
+    unsigned int _7b0 = 0, _7d0 = 0, e8b = 0, tmp;
 
     /* Collect diagnostics about available mitigations. */
     if ( boot_cpu_data.cpuid_level >= 7 )
-        cpuid_count(7, 0, &tmp, &tmp, &tmp, &_7d0);
+        cpuid_count(7, 0, &tmp, &_7b0, &tmp, &_7d0);
     if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )
         cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);
 
     printk(XENLOG_DEBUG "Speculative mitigation facilities:\n");
 
     /* Hardware features which pertain to speculative mitigations. */
-    if ( (_7d0 & (cpufeat_mask(X86_FEATURE_IBRSB) |
+    if ( (_7b0 & cpufeat_mask(X86_FEATURE_SMEP)) ||
+         (_7d0 & (cpufeat_mask(X86_FEATURE_IBRSB) |
                   cpufeat_mask(X86_FEATURE_STIBP))) ||
          (e8b & cpufeat_mask(X86_FEATURE_IBPB)) )
-        printk(XENLOG_DEBUG "  Hardware features:%s%s%s\n",
+        printk(XENLOG_DEBUG "  Hardware features:%s%s%s%s\n",
+               (_7b0 & cpufeat_mask(X86_FEATURE_SMEP))  ? " SMEP"      : "",
                (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB)) ? " IBRS/IBPB" : "",
                (_7d0 & cpufeat_mask(X86_FEATURE_STIBP)) ? " STIBP"     : "",
                (e8b  & cpufeat_mask(X86_FEATURE_IBPB))  ? " IBPB"      : "");
@@ -96,13 +103,18 @@ static void __init print_details(enum ind_thunk thunk)
         printk(XENLOG_DEBUG "  Compiled-in support: INDIRECT_THUNK\n");
 
     printk(XENLOG_INFO
-           "BTI mitigations: Thunk %s, Others:%s\n",
+           "BTI mitigations: Thunk %s, Others:%s%s%s%s\n",
            thunk == THUNK_NONE      ? "N/A" :
            thunk == THUNK_RETPOLINE ? "RETPOLINE" :
            thunk == THUNK_LFENCE    ? "LFENCE" :
            thunk == THUNK_JMP       ? "JMP" : "?",
            boot_cpu_has(X86_FEATURE_XEN_IBRS_SET)    ? " IBRS+" :
-           boot_cpu_has(X86_FEATURE_XEN_IBRS_CLEAR)  ? " IBRS-"      : "");
+           boot_cpu_has(X86_FEATURE_XEN_IBRS_CLEAR)  ? " IBRS-"      : "",
+           cpu_has_xen_smep                          ? " SMEP"       : "",
+           (boot_cpu_has(X86_FEATURE_RSB_VMEXIT) ||
+            boot_cpu_has(X86_FEATURE_RSB_VMEXIT_SS)) ? " RSB_VMEXIT" : "",
+           (boot_cpu_has(X86_FEATURE_RSB_NATIVE) ||
+            boot_cpu_has(X86_FEATURE_RSB_NATIVE_SS)) ? " RSB_NATIVE" : "");
 }
 
 /* Calculate whether Retpoline is known-safe on this CPU. */
@@ -162,13 +174,14 @@ static bool __init retpoline_safe(void)
 void __init init_speculation_mitigations(void)
 {
     enum ind_thunk thunk = THUNK_DEFAULT;
-    bool ibrs = false;
+    bool ibrs = false, have_mitigation = true;
 
     /*
      * Has the user specified any custom BTI mitigations?  If so, follow their
      * instructions exactly and disable all heuristics.
      */
-    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )
+    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 ||
+         opt_rsb_native != -1 || opt_rsb_vmexit != -1 )
     {
         thunk = opt_thunk;
         ibrs  = !!opt_ibrs;
@@ -202,6 +215,9 @@ void __init init_speculation_mitigations(void)
         /* Without compiler thunk support, use IBRS if available. */
         else if ( boot_cpu_has(X86_FEATURE_IBRSB) )
             ibrs = true;
+        /* Or give up completely. */
+        else
+            have_mitigation = false;
     }
 
     /*
@@ -243,6 +259,54 @@ void __init init_speculation_mitigations(void)
             setup_force_cpu_cap(X86_FEATURE_XEN_IBRS_CLEAR);
     }
 
+    /*
+     * Only bother overwriting the RSBs if we have a BTI mitigation available.
+     * Otherwise, we're already wide open to easier attacks than RSB-poisoning.
+     */
+    if ( have_mitigation )
+    {
+        /*
+         * If we are writing to MSR_SPEC_CTRL, the WRMSR is sufficiently
+         * serialising to protect against speculative exits of the RSB loop.
+         * If not, the RSB loop needs to provide its own speculative defence.
+         */
+        bool ss = !(boot_cpu_has(X86_FEATURE_XEN_IBRS_SET) ||
+                    boot_cpu_has(X86_FEATURE_XEN_IBRS_CLEAR));
+
+        /*
+         * HVM guests can always poison the RSB to point at Xen supervisor
+         * mappings.
+         */
+        if ( opt_rsb_vmexit )
+        {
+            BUILD_BUG_ON(X86_FEATURE_RSB_VMEXIT_SS !=
+                         X86_FEATURE_RSB_VMEXIT + 1);
+
+            setup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT + ss);
+        }
+
+        /*
+         * PV guests can poison the RSB to any virtual address from which
+         * they can execute a call instruction.  This is necessarily outside
+         * of the Xen supervisor mappings.
+         *
+         * With SMEP enabled, the processor won't speculate into user
+         * mappings.  Therefore, we don't need to worry about poisoned
+         * entries from 64bit PV guests.
+         *
+         * 32bit PV guest kernels run in ring 1, so use supervisor mappings.
+         * If a processors speculates to 32bit PV guest kernel mappings, it is
+         * speculating in 64bit supervisor mode, and can leak data.
+         */
+        if ( opt_rsb_native )
+        {
+            BUILD_BUG_ON(X86_FEATURE_RSB_NATIVE_SS !=
+                         X86_FEATURE_RSB_NATIVE + 1);
+
+            setup_force_cpu_cap(X86_FEATURE_RSB_NATIVE + ss);
+        }
+    }
+
     print_details(thunk);
 }
 
diff --git a/xen/include/asm-x86/cpufeature.h b/xen/include/asm-x86/cpufeature.h
index f41352d..8bbaddf 100644
--- a/xen/include/asm-x86/cpufeature.h
+++ b/xen/include/asm-x86/cpufeature.h
@@ -32,6 +32,10 @@
 #define X86_FEATURE_XEN_IBPB		((FSCAPINTS+0)*32+15) /* IBRSB || IBPB */
 #define X86_FEATURE_XEN_IBRS_SET	((FSCAPINTS+0)*32+16) /* IBRSB && IRBS set in Xen */
 #define X86_FEATURE_XEN_IBRS_CLEAR	((FSCAPINTS+0)*32+17) /* IBRSB && IBRS clear in Xen */
+#define X86_FEATURE_RSB_NATIVE		((FSCAPINTS+0)*32+18) /* RSB overwrite needed for native */
+#define X86_FEATURE_RSB_NATIVE_SS	((FSCAPINTS+0)*32+19) /* RSB_NATIVE must self-serialise */
+#define X86_FEATURE_RSB_VMEXIT		((FSCAPINTS+0)*32+20) /* RSB overwrite needed for vmexit */
+#define X86_FEATURE_RSB_VMEXIT_SS	((FSCAPINTS+0)*32+21) /* RSB_VMEXIT must self-serialise */
 
 #define cpufeat_word(idx)	((idx) / 32)
 #define cpufeat_bit(idx)	((idx) % 32)
@@ -98,6 +102,7 @@
 #define cpu_has_hle		boot_cpu_has(X86_FEATURE_HLE)
 #define cpu_has_rtm		boot_cpu_has(X86_FEATURE_RTM)
 #define cpu_has_pdcm		boot_cpu_has(X86_FEATURE_PDCM)
+#define cpu_has_xen_smep        cpu_has_smep
 #define cpu_has_lfence_dispatch boot_cpu_has(X86_FEATURE_LFENCE_DISPATCH)
 #define cpu_has_stibp           boot_cpu_has(X86_FEATURE_STIBP)
 
diff --git a/xen/include/asm-x86/nops.h b/xen/include/asm-x86/nops.h
index d048843..e1fbdef 100644
--- a/xen/include/asm-x86/nops.h
+++ b/xen/include/asm-x86/nops.h
@@ -65,9 +65,11 @@
 
 #define ASM_NOP22 ASM_NOP8 ASM_NOP8 ASM_NOP6
 #define ASM_NOP26 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP2
+#define ASM_NOP27 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP3
 #define ASM_NOP32 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP8
 #define ASM_NOP33 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP7 ASM_NOP2
 #define ASM_NOP39 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP7
+#define ASM_NOP40 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP8 ASM_NOP8
 
 #define ASM_NOP_MAX 8
 
diff --git a/xen/include/asm-x86/spec_ctrl_asm.h b/xen/include/asm-x86/spec_ctrl_asm.h
index 13e058c..430b440 100644
--- a/xen/include/asm-x86/spec_ctrl_asm.h
+++ b/xen/include/asm-x86/spec_ctrl_asm.h
@@ -73,6 +73,37 @@
  *  - SPEC_CTRL_EXIT_TO_GUEST
  */
 
+.macro DO_OVERWRITE_RSB maybexen:req ss:req
+/*
+ * Req: %rsp=regs
+ * Clobbers %ecx
+ *
+ * Requires 256 bytes of stack space, but %rsp has no net change.  Optionally
+ * checks for interrupting Xen context, and skipping the clobber.
+ *
+ * For safety, there must be an instruction stream serialising event between
+ * this loop and the next unmatched ret, to prevent an early speculative exit.
+ * If IBRS is in use, its WRMSR is sufficiently serialising.  If IBRS is not
+ * available, place an lfence after the loop to seriailse.
+ */
+    .if \maybexen
+        cmpl $__HYPERVISOR_CS, UREGS_cs(%rsp)
+        je .Lend_\@
+    .endif
+
+    mov $32, %ecx
+.Lloop_\@: call .Lcall_\@
+    pause
+.Lcall_\@: sub $1, %ecx
+    jnz .Lloop_\@
+    add $32*8, %rsp
+.Lend_\@:
+
+    .if \ss /* Need to self-serialise? */
+        lfence
+    .endif
+.endm
+
 .macro DO_SPEC_CTRL_ENTRY_FROM_VMEXIT ibrs_val:req
 /*
  * Requires %rbx=current, %rsp=regs/cpuinfo
@@ -178,6 +209,11 @@
 
 /* Use after a VMEXIT from an HVM guest. */
 #define SPEC_CTRL_ENTRY_FROM_VMEXIT                                     \
+    ALTERNATIVE_2 __stringify(ASM_NOP27),                               \
+        "DO_OVERWRITE_RSB maybexen=0 ss=1",                             \
+        X86_FEATURE_RSB_VMEXIT_SS,                                      \
+        "DO_OVERWRITE_RSB maybexen=0 ss=0",                             \
+        X86_FEATURE_RSB_VMEXIT;                                         \
     ALTERNATIVE_2 __stringify(ASM_NOP32),                               \
         __stringify(DO_SPEC_CTRL_ENTRY_FROM_VMEXIT                      \
                     ibrs_val=SPEC_CTRL_IBRS),                           \
@@ -188,6 +224,11 @@
 
 /* Use after an entry from PV context (syscall/sysenter/int80/int82/etc). */
 #define SPEC_CTRL_ENTRY_FROM_PV                                         \
+    ALTERNATIVE_2 __stringify(ASM_NOP27),                               \
+        "DO_OVERWRITE_RSB maybexen=0 ss=1",                             \
+        X86_FEATURE_RSB_NATIVE_SS,                                      \
+        "DO_OVERWRITE_RSB maybexen=0 ss=0",                             \
+        X86_FEATURE_RSB_NATIVE;                                         \
     ALTERNATIVE_2 __stringify(ASM_NOP22),                               \
         __stringify(DO_SPEC_CTRL_ENTRY maybexen=0                       \
                     ibrs_val=SPEC_CTRL_IBRS),                           \
@@ -197,6 +238,11 @@
 
 /* Use in interrupt/exception context.  May interrupt Xen or PV context. */
 #define SPEC_CTRL_ENTRY_FROM_INTR                                       \
+    ALTERNATIVE_2 __stringify(ASM_NOP40),                               \
+        "DO_OVERWRITE_RSB maybexen=1 ss=1",                             \
+        X86_FEATURE_RSB_NATIVE_SS,                                      \
+        "DO_OVERWRITE_RSB maybexen=1 ss=0",                             \
+        X86_FEATURE_RSB_NATIVE;                                         \
     ALTERNATIVE_2 __stringify(ASM_NOP39),                               \
         __stringify(DO_SPEC_CTRL_ENTRY maybexen=1                       \
                     ibrs_val=SPEC_CTRL_IBRS),                           \
