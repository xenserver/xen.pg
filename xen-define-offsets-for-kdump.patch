Cause to be defined all those extra Xen symbol offsets that kdump needs
to navigate its way around a memory image.

diff -r 9defadb2396a xen/Makefile
--- a/xen/Makefile
+++ b/xen/Makefile
@@ -191,6 +191,7 @@ _cscope:
 .PHONY: _MAP
 _MAP:
 	$(NM) -n $(TARGET)-syms | grep -v '\(compiled\)\|\(\.o$$\)\|\( [aUw] \)\|\(\.\.ng$$\)\|\(LASH[RL]DI\)' > System.map
+	cat include/asm/asm-offsets.h | awk '/^#define __ASM_OFFSETS_H__/ { next } ; /^#define / { printf "%016x - +%s\n", $$3, $$2 }' >> System.map
 
 .PHONY: FORCE
 FORCE:
diff -r 9defadb2396a xen/arch/x86/x86_64/asm-offsets.c
--- a/xen/arch/x86/x86_64/asm-offsets.c
+++ b/xen/arch/x86/x86_64/asm-offsets.c
@@ -54,8 +54,49 @@ void __dummy__(void)
     DEFINE(UREGS_user_sizeof, sizeof(struct cpu_user_regs));
     BLANK();
 
+    OFFSET(DOMAIN_id, struct domain, domain_id);
+    OFFSET(DOMAIN_shared_info, struct domain, shared_info);
+    OFFSET(DOMAIN_next, struct domain, next_in_list);
+    OFFSET(DOMAIN_max_vcpus, struct domain, max_vcpus);
+    OFFSET(DOMAIN_vcpus, struct domain, vcpu);
+    OFFSET(DOMAIN_is_hvm, struct domain, is_hvm);
+    OFFSET(DOMAIN_is_privileged, struct domain, is_privileged);
+    OFFSET(DOMAIN_tot_pages, struct domain, tot_pages);
+    OFFSET(DOMAIN_max_pages, struct domain, max_pages);
+    OFFSET(DOMAIN_shr_pages, struct domain, shr_pages);
+    OFFSET(DOMAIN_has_32bit_shinfo, struct domain, arch.has_32bit_shinfo);
+    OFFSET(DOMAIN_handle, struct domain, handle);
+    OFFSET(DOMAIN_paging_mode, struct domain, arch.paging.mode);
+    DEFINE(DOMAIN_sizeof, sizeof(struct domain));
+    BLANK();
+
+    OFFSET(SHARED_max_pfn, struct shared_info, arch.max_pfn);
+    OFFSET(SHARED_pfn_to_mfn_list_list, struct shared_info, arch.pfn_to_mfn_frame_list_list);
+    BLANK();
+
+    DEFINE(VIRT_XEN_START, XEN_VIRT_START);
+    DEFINE(VIRT_XEN_END, XEN_VIRT_END);
+    DEFINE(VIRT_DIRECTMAP_START, DIRECTMAP_VIRT_START);
+    DEFINE(VIRT_DIRECTMAP_END, DIRECTMAP_VIRT_END);
+
+    DEFINE(XEN_DEBUG, debug_build());
+    DEFINE(XEN_STACK_SIZE, STACK_SIZE);
+    DEFINE(XEN_PRIMARY_STACK_SIZE, PRIMARY_STACK_SIZE);
+#ifdef MEMORY_GUARD
+    DEFINE(XEN_MEMORY_GUARD, 1);
+#else
+    DEFINE(XEN_MEMORY_GUARD, 0);
+#endif
+#ifdef CONFIG_FRAME_POINTER
+    DEFINE(XEN_FRAME_POINTER, 1);
+#else
+    DEFINE(XEN_FRAME_POINTER, 0);
+#endif
+    BLANK();
+
     OFFSET(irq_caps_offset, struct domain, irq_caps);
     OFFSET(next_in_list_offset, struct domain, next_in_list);
+    OFFSET(VCPU_vcpu_id, struct vcpu, vcpu_id);
     OFFSET(VCPU_processor, struct vcpu, processor);
     OFFSET(VCPU_domain, struct vcpu, domain);
     OFFSET(VCPU_vcpu_info, struct vcpu, vcpu_info);
@@ -86,7 +127,10 @@ void __dummy__(void)
     OFFSET(VCPU_kernel_sp, struct vcpu, arch.pv_vcpu.kernel_sp);
     OFFSET(VCPU_kernel_ss, struct vcpu, arch.pv_vcpu.kernel_ss);
     OFFSET(VCPU_guest_context_flags, struct vcpu, arch.vgc_flags);
+    OFFSET(VCPU_user_regs, struct vcpu, arch.user_regs);
+    OFFSET(VCPU_cr3, struct vcpu, arch.cr3);
     OFFSET(VCPU_nmi_pending, struct vcpu, nmi_pending);
+    OFFSET(VCPU_pause_flags, struct vcpu, pause_flags);
     OFFSET(VCPU_mce_pending, struct vcpu, mce_pending);
     OFFSET(VCPU_nmi_old_mask, struct vcpu, nmi_state.old_mask);
     OFFSET(VCPU_mce_old_mask, struct vcpu, mce_state.old_mask);
@@ -95,6 +139,7 @@ void __dummy__(void)
     DEFINE(VCPU_TRAP_MCE, VCPU_TRAP_MCE);
     DEFINE(_VGCF_failsafe_disables_events, _VGCF_failsafe_disables_events);
     DEFINE(_VGCF_syscall_disables_events,  _VGCF_syscall_disables_events);
+    DEFINE(VCPU_sizeof, sizeof(struct vcpu));
     BLANK();
 
     OFFSET(VCPU_svm_vmcb_pa, struct vcpu, arch.hvm_svm.vmcb_pa);
@@ -134,6 +179,7 @@ void __dummy__(void)
     OFFSET(CPUINFO_guest_cpu_user_regs, struct cpu_info, guest_cpu_user_regs);
     OFFSET(CPUINFO_processor_id, struct cpu_info, processor_id);
     OFFSET(CPUINFO_current_vcpu, struct cpu_info, current_vcpu);
+    OFFSET(CPUINFO_per_cpu_offset, struct cpu_info, per_cpu_offset);
     DEFINE(CPUINFO_sizeof, sizeof(struct cpu_info));
     BLANK();
 
